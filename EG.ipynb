{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d45c871b-2dab-4874-bd22-438e5889b297",
   "metadata": {},
   "source": [
    " Here, we enhance the original graph by introducing additional edges based on the TF-IDF vector similarity between each pair of nodes. We approximate the nearest neighbors for each node and utilize the resulting enhanced graph to create the graph vectors. The classifier is then trained using these vectors to predict the labels for new incoming edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ac437-c096-42df-aea9-84c1aebc4d4e",
   "metadata": {},
   "source": [
    "## Offline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366cd93-9d87-46bd-ac07-846cf15ee539",
   "metadata": {},
   "source": [
    "### requires text_vectors.pkl which is ommited by runing D.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a786c56-f7c3-472a-80de-bb0bb5af1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pandas as pd\n",
    "sparsevectors={}\n",
    "with open('text_vectors.pkl', 'rb') as f:\n",
    "    sparsevectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248587d2-27f8-40fa-be05-1786d213dfad",
   "metadata": {},
   "source": [
    "build index for aproximate Nearest Neighboards using tf-idf vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e7ffa-4518-4e70-b90c-28718e7dfdfa",
   "metadata": {},
   "source": [
    "Install pysparn\n",
    "\n",
    "git clone https://github.com/facebookresearch/pysparnn.git\n",
    "\n",
    "cd pysparnn && python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d07a4b-8bbd-4eb1-ba47-0885fb402a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysparnn.cluster_index as ci\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "dfdesc= pd.read_csv('nodes/nodes.tsv', sep='\\t', header=0)\n",
    "\n",
    "dfdesc=dfdesc.set_index('id')\n",
    "\n",
    "dfdesc = dfdesc.fillna(\"\")\n",
    "corpus = dfdesc.values[:,0]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "Xtfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "\n",
    "cp = ci.MultiClusterIndex(Xtfidf, dfdesc.index)\n",
    "\n",
    "\n",
    "with open('cluterIndex.bin', 'wb') as fh_out:\n",
    "    pickle.dump(cp, fh_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf39a6d-c330-446e-b896-c2bb58ef045d",
   "metadata": {},
   "source": [
    "Load Index and perform queries to build new edges (and store them in new_edges.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917de69c-e79f-42ca-8f2c-62c4cfa48521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsimilartups(idn):\n",
    "    res=cp2.search(sparsevectors[idn], k=25, k_clusters=1, return_distance=True)\n",
    "    tups=[ (iddd[1],idn) for iddd in res[0][1:] if iddd[0]<0.90]\n",
    "    with open(\"new_edges.txt\",\"a+\") as f:\n",
    "        for tup in tups:\n",
    "            f.write(f\"{tup[0]},{tup[1]}\\n\")\n",
    "\n",
    "with open('cluterIndex.bin', 'rb') as file_:\n",
    "    cp2 =pickle.load(file_)\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from multiprocessing import Pool\n",
    "    start = time.time()\n",
    "    with Pool(avpool) as p:\n",
    "        data=p.map(getsimilartups, list(sparsevectors.keys()))\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b63ec-43fd-4db8-9373-58a4e8181ad7",
   "metadata": {},
   "source": [
    "Build the enhanced graph using new_edges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2e227d0-e833-411e-b744-c678dad9cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "df_edges=pd.read_csv('train.csv')\n",
    "\n",
    "df_edges_all = df_edges[['id1', 'id2']]\n",
    "\n",
    "sparsevectors={}\n",
    "with open('text_vectors.pkl', 'rb') as f:\n",
    "    sparsevectors = pickle.load(f)\n",
    "# load edges from tzt file (new_edges.txt)\n",
    "dfouredges=pd.read_csv(\"new_edges.txt\",header=None,names=[\"id1\",\"id2\"])\n",
    "\n",
    "dfouredges[\"id1\"]=dfouredges[\"id1\"].astype(int)\n",
    "dfouredges[\"id2\"]=dfouredges[\"id2\"].astype(int)\n",
    "\n",
    "df_edges_all=pd.concat([df_edges_all, dfouredges], ignore_index=True)\n",
    "\n",
    "\n",
    "nodes=list(sparsevectors.keys())\n",
    "\n",
    "\n",
    "\n",
    "G= nx.Graph()\n",
    "#add nodes\n",
    "G.add_nodes_from(nodes)\n",
    "#add edges\n",
    "edges = [(row.id1,row.id2) for row in df_edges[df_edges[\"label\"]==1].itertuples()]\n",
    "G.add_edges_from(edges)\n",
    "edges2 = [(row.id1,row.id2) for row in dfouredges.itertuples()]\n",
    "G.add_edges_from(edges2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b2fb1-96d0-488b-bc3d-47bfc97029b8",
   "metadata": {},
   "source": [
    "Define graph metrics (same as G solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad7320f9-44f7-43e8-8705-0c2d2304f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Adding methods proposed in the https://link.springer.com/content/pdf/10.1038/s41598-019-57304-y.pdf\n",
    "def shortest_path(G:nx.Graph,source:int,target:int)->float:\n",
    "    try:\n",
    "        p=nx.shortest_path(G,source=source,target=target)\n",
    "        return len(p)-1\n",
    "    except nx.NetworkXNoPath:\n",
    "        return float(\"inf\")\n",
    "\n",
    "def distance(G:nx.Graph,source:int,target:int)->float:\n",
    "   '''\n",
    "   This method can be changed to any distance\n",
    "   '''\n",
    "   return shortest_path(G,source,target)\n",
    "\n",
    "def closeness_centrality(G:nx.Graph, source:int, target:int)->float:\n",
    "    dxy = nx.shortest_path(G,source=source,target=target)\n",
    "    return G.number_of_nodes()/shortest_path(G,source,target)\n",
    "\n",
    "def common_neighbors(G:nx.Graph, source:int, target:int)->list:\n",
    "    s_neighbors = list(G.adj[source].keys())\n",
    "    t_neighbors = list(G.adj[target].keys())\n",
    "    common_neighbors = set(s_neighbors).intersection(t_neighbors)\n",
    "    return common_neighbors\n",
    "\n",
    "def CCPA(G:nx.Graph, source:int, target:int,a:float = 0.5)->float:\n",
    "    '''\n",
    "    Common Neighbor and Centrality based Parameterized Algorithm\n",
    "    '''\n",
    "    return a*closeness_centrality(G,source,target)+(1-a)*len(common_neighbors(G,source,target))\n",
    "\n",
    "def CND(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Common Neighbor and Distance\n",
    "    '''\n",
    "    cn = common_neighbors(G,source,target)\n",
    "    if len(cn)>0:\n",
    "        return (len(cn)+1)/2\n",
    "    else:\n",
    "        if distance(G,source,target)==0:\n",
    "            return 0\n",
    "        return 1/distance(G,source,target)\n",
    "\n",
    "def PA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Preferential Attachment\n",
    "    '''\n",
    "    return G.degree[source]*G.degree[target]\n",
    "\n",
    "def AA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Adamic Adar\n",
    "    '''\n",
    "    similarity = 0.0\n",
    "    for neighbor in common_neighbors(G,source,target):\n",
    "        degree = G.degree[neighbor]\n",
    "        if degree > 1:\n",
    "            similarity += 1 / math.log(degree)\n",
    "    return similarity\n",
    "\n",
    "def CN(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Common Neighbor\n",
    "    '''\n",
    "    return len(common_neighbors(G,source,target))\n",
    "\n",
    "def SI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Sorensen Index\n",
    "    '''\n",
    "    if (G.degree[source]+G.degree[target]) ==0:\n",
    "        return 0\n",
    "    return 2*CN(G,source,target)/(G.degree[source]+G.degree[target])\n",
    "\n",
    "def JI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Jaccard Index\n",
    "    '''\n",
    "    s_neighbors = list(G.adj[source].keys())\n",
    "    t_neighbors = list(G.adj[target].keys())\n",
    "    common_neighbors = set(s_neighbors).intersection(t_neighbors)\n",
    "    all_neighbors = set(s_neighbors).union(t_neighbors)\n",
    "    if len(all_neighbors)==0:\n",
    "        return 0\n",
    "    return len(common_neighbors)/len(all_neighbors)\n",
    "\n",
    "def RA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Resource Allocation\n",
    "    '''\n",
    "    similarity = 0.0\n",
    "    for neighbor in common_neighbors(G,source,target):\n",
    "        degree = G.degree[neighbor]\n",
    "        if degree > 1:\n",
    "            similarity += 1 / degree\n",
    "    return similarity\n",
    "\n",
    "def HPI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Hub Promoted Index\n",
    "    '''\n",
    "    if min([G.degree[source],G.degree[target]]) ==0 :\n",
    "        return 0\n",
    "    return CN(G,source,target)/min([G.degree[source],G.degree[target]])\n",
    "\n",
    "def create_metric_vector(tup):\n",
    "    G=tup[0]\n",
    "    source= tup[1]\n",
    "    target= tup[2]\n",
    "    functions = [CND, PA, AA, CN, SI, JI, RA, HPI]\n",
    "    outputs = []\n",
    "    for func in functions:\n",
    "        output = func(G, source, target)\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def makedatasetapply(df):\n",
    "    label=df.label\n",
    "    datadf = df.apply(lambda row: create_metric_vector((G,row['id1'],row['id2'])), axis=1,result_type='expand')\n",
    "    return datadf,label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a7f16-f080-4b63-99f3-25ebf41d2604",
   "metadata": {},
   "source": [
    "Create vectors for training data using enhanced graph and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0066206-2ab5-44d2-90fc-11e9d2b0c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=makedatasetapply(df_edges)\n",
    "\n",
    "Xcopy=X.copy()\n",
    "Xcopy[\"label\"]=Y\n",
    "Xcopy.to_csv(\"enhance_vectors_with_labels.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1e4d9-3ab2-4773-b8ac-ce7b227e1476",
   "metadata": {},
   "source": [
    "Train classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b703f062-502e-4072-aed6-28922bb270aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xcopy=pd.read_csv(\"enhance_vectors_with_labels.csv\")\n",
    "Y=Xcopy[\"label\"]\n",
    "X=Xcopy.drop([\"label\"],axis=1)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625a027-b7c0-4020-b802-50740f3bf8d7",
   "metadata": {},
   "source": [
    "## Online"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce494d2-83ae-452d-a085-98069f4c54e6",
   "metadata": {},
   "source": [
    "Create test vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f527b6c7-8b70-4f58-8f55-7cdabaca9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedataset_TEST(df):\n",
    "    datadf = df.apply(lambda row: create_metric_vector((G,row['id1'],row['id2'])), axis=1,result_type='expand')\n",
    "    return datadf\n",
    "\n",
    "df_edges_all_test=pd.read_csv('test.csv')\n",
    "X_test=makedataset_TEST(df_edges_all_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10b8508c-d8ba-47b6-a513-71dc433aaa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcopy=X_test.copy()\n",
    "Xcopy.to_csv(\"enhance_vectors_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f9c69-9fc2-40de-a97f-9fc70ed18b03",
   "metadata": {},
   "source": [
    "produce predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd7fa300-6e65-4d9d-b330-988c7d9d1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf3561-eadf-4f61-8242-ca59ab445410",
   "metadata": {},
   "source": [
    "Store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8805997e-95bb-4e32-b341-e71671c9f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexpe=pd.DataFrame(zip(df_edges_all_test[\"id\"].values,preds),columns=[\"id\",\"label\"])\n",
    "dfexpe.to_csv(\"enhance_graph_submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
