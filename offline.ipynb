{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a6d353-5fbe-4341-9037-a5d4495cf62f",
   "metadata": {},
   "source": [
    "## Offline time of TF-IDF in all descriptions (nodes), plus similarities of training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb52e44-6c62-4b26-97f9-08913f69fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "avpool=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f55b91-4810-446d-8cf7-ad002e62b5ce",
   "metadata": {},
   "source": [
    "Time of tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b14ebde-7cc1-4f8f-8558-279e8aa182b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "dfdesc= pd.read_csv('nodes/nodes.tsv', sep='\\t', header=0)\n",
    "\n",
    "dfdesc=dfdesc.set_index('id')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "start = time.time()\n",
    "dfdesc = dfdesc.fillna(\"\")\n",
    "corpus = dfdesc.values[:,0]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "Xtfidf = vectorizer.fit_transform(corpus)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b588ff52-926a-4a8b-9fef-ab7284022f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 837834/837834 [01:00<00:00, 13880.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# write in file\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "sparsevectors={}\n",
    "\n",
    "for i in tqdm(range(X.shape[0])):\n",
    "    sparsevectors[dfdesc.index[i]]=Xtfidf[i]\n",
    "\n",
    "import pickle \n",
    "\n",
    "with open('text_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(sparsevectors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea74e0-2839-4408-a9ff-b70ae9394a3d",
   "metadata": {},
   "source": [
    "Time for pair similarities in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a19e70e-cd0e-48d7-8c1c-14693a3969d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9482it [00:00, 23603.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.718411445617676\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'get_indexer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14457/415027613.py\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mdfsimilarities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_edges_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"similarity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mdfsimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_similarities.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     arrays, columns, index = nested_data_to_arrays(\n\u001b[0m\u001b[1;32m    747\u001b[0m                         \u001b[0;31m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                         \u001b[0;31m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_of_dict_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_of_series_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;31m# last ditch effort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_list_of_series_to_arrays\u001b[0;34m(data, columns)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'get_indexer'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df_edges_test=pd.read_csv('train.csv')\n",
    "#df_edges_test= df_edges_test.iloc[:int(len(df_edges_test)*0.01)]\n",
    "\n",
    "def distance_description(tup)->float:\n",
    "    '''\n",
    "    This method can be changed to any distance\n",
    "    '''\n",
    "    source=tup[0]\n",
    "    target=tup[1]\n",
    "    sp1=sparsevectors[source]\n",
    "    sp2=sparsevectors[target]\n",
    "    \n",
    "    return cosine_similarity(sp1,sp2)[0][0]\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "my_preds=[]\n",
    "start = time.time()\n",
    "\n",
    "tups=[]\n",
    "for ind, row in tqdm(df_edges_test.iterrows()):\n",
    "    tups.append((row[\"id1\"],row[\"id2\"]))\n",
    "with Pool(avpool) as p:\n",
    "    data=p.map(distance_description, tups)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    dfsimilarities=pd.DataFrame(zip(df_edges_test[\"id\"],data),columns=[\"id\",\"similarity\"])\n",
    "    dfsimilarities.to_csv(\"train_similarities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf3bc2-c537-40fa-9c98-b5c0f8333cd2",
   "metadata": {},
   "source": [
    "## Time to build graph amd vector calculation in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8510cd97-577c-4bca-942b-c5b735adab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import train dataset\n",
    "import pandas as pd\n",
    "\n",
    "df_edges_all=pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "df_edges= df_edges_all\n",
    "\n",
    "import pickle \n",
    "import pandas as pd\n",
    "sparsevectors={}\n",
    "with open('text_vectors.pkl', 'rb') as f:\n",
    "    sparsevectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828a2bb2-177e-4667-a374-b090b05455fb",
   "metadata": {},
   "source": [
    "Time for building graph in train nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f722a9d1-5b02-4041-bf30-5606d922c3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.832526922225952\n"
     ]
    }
   ],
   "source": [
    "# The existing graph\n",
    "import networkx as nx\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "G= nx.Graph()\n",
    "#add nodes\n",
    "nodes = pd.unique(df_edges_all[['id1', 'id2']].values.ravel())\n",
    "G.add_nodes_from(nodes)\n",
    "#add edges\n",
    "edges = [(row.id1,row.id2) for row in df_edges[df_edges[\"label\"]==1].itertuples()]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# keeping all the negative edges so we can compare results\n",
    "no_edges = [(row.id1,row.id2) for row in df_edges[df_edges[\"label\"]==0].itertuples()]\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59eed423-408a-447c-895c-a9007421a426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# Adding methods proposed in the https://link.springer.com/content/pdf/10.1038/s41598-019-57304-y.pdf\n",
    "def shortest_path(G:nx.Graph,source:int,target:int)->float:\n",
    "    try:\n",
    "        p=nx.shortest_path(G,source=source,target=target)\n",
    "        return len(p)-1\n",
    "    except nx.NetworkXNoPath:\n",
    "        return float(\"inf\")\n",
    "\n",
    "def distance(G:nx.Graph,source:int,target:int)->float:\n",
    "   '''\n",
    "   This method can be changed to any distance\n",
    "   '''\n",
    "   return shortest_path(G,source,target)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def distance_description(G,source:int,target:int)->float:\n",
    "    '''\n",
    "    This method can be changed to any distance\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    sp1=sparsevectors[source]\n",
    "    sp2=sparsevectors[target]\n",
    "    \n",
    "    return cosine_similarity(sp1,sp2)[0][0]\n",
    "\n",
    "\n",
    "def closeness_centrality(G:nx.Graph, source:int, target:int)->float:\n",
    "    dxy = nx.shortest_path(G,source=source,target=target)\n",
    "    return G.number_of_nodes()/shortest_path(G,source,target)\n",
    "\n",
    "def common_neighbors(G:nx.Graph, source:int, target:int)->list:\n",
    "    s_neighbors = list(G.adj[source].keys())\n",
    "    t_neighbors = list(G.adj[target].keys())\n",
    "    common_neighbors = set(s_neighbors).intersection(t_neighbors)\n",
    "    return common_neighbors\n",
    "\n",
    "def CCPA(G:nx.Graph, source:int, target:int,a:float = 0.5)->float:\n",
    "    '''\n",
    "    Common Neighbor and Centrality based Parameterized Algorithm\n",
    "    '''\n",
    "    return a*closeness_centrality(G,source,target)+(1-a)*len(common_neighbors(G,source,target))\n",
    "\n",
    "def CND(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Common Neighbor and Distance\n",
    "    '''\n",
    "    cn = common_neighbors(G,source,target)\n",
    "    if len(cn)>0:\n",
    "        return (len(cn)+1)/2\n",
    "    else:\n",
    "        if distance(G,source,target)==0:\n",
    "            return 0\n",
    "        return 1/distance(G,source,target)\n",
    "\n",
    "def PA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Preferential Attachment\n",
    "    '''\n",
    "    return G.degree[source]*G.degree[target]\n",
    "\n",
    "def AA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Adamic Adar\n",
    "    '''\n",
    "    similarity = 0.0\n",
    "    for neighbor in common_neighbors(G,source,target):\n",
    "        degree = G.degree[neighbor]\n",
    "        if degree > 1:\n",
    "            similarity += 1 / math.log(degree)\n",
    "    return similarity\n",
    "\n",
    "def CN(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Common Neighbor\n",
    "    '''\n",
    "    return len(common_neighbors(G,source,target))\n",
    "\n",
    "def SI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Sorensen Index\n",
    "    '''\n",
    "    if (G.degree[source]+G.degree[target]) ==0:\n",
    "        return 0\n",
    "    return 2*CN(G,source,target)/(G.degree[source]+G.degree[target])\n",
    "\n",
    "def JI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Jaccard Index\n",
    "    '''\n",
    "    s_neighbors = list(G.adj[source].keys())\n",
    "    t_neighbors = list(G.adj[target].keys())\n",
    "    common_neighbors = set(s_neighbors).intersection(t_neighbors)\n",
    "    all_neighbors = set(s_neighbors).union(t_neighbors)\n",
    "    if len(all_neighbors)==0:\n",
    "        return 0\n",
    "    return len(common_neighbors)/len(all_neighbors)\n",
    "\n",
    "def RA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Resource Allocation\n",
    "    '''\n",
    "    similarity = 0.0\n",
    "    for neighbor in common_neighbors(G,source,target):\n",
    "        degree = G.degree[neighbor]\n",
    "        if degree > 1:\n",
    "            similarity += 1 / degree\n",
    "    return similarity\n",
    "\n",
    "def HPI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Hub Promoted Index\n",
    "    '''\n",
    "    if min([G.degree[source],G.degree[target]]) ==0 :\n",
    "        return 0\n",
    "    return CN(G,source,target)/min([G.degree[source],G.degree[target]])\n",
    "\n",
    "def create_metric_vector(tup):\n",
    "    G=tup[0]\n",
    "    source= tup[1]\n",
    "    target= tup[2]\n",
    "    functions = [CND, PA, AA, CN, SI, JI, RA, HPI]\n",
    "    outputs = []\n",
    "    for func in functions:\n",
    "        output = func(G, source, target)\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def makedataset(df):\n",
    "    label=df.label\n",
    "    data=[]\n",
    "    allind=len(df.index)\n",
    "    counter=0\n",
    "    tups=[]\n",
    "    for i in tqdm(range(len(df.index))):\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        tups.append((G,row.id1,row.id2))\n",
    "    with Pool(avpool) as p:\n",
    "        data=p.map(create_metric_vector, tups)\n",
    "    \n",
    "    datadf=pd.DataFrame(data)\n",
    "    return datadf,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "890392db-1440-4a1f-a51b-8a2470f3ed99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 948232/948232 [00:43<00:00, 21553.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.30477118492126\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "X,Y=makedataset(df_edges)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "Xcopy=X.copy()\n",
    "Xcopy[\"label\"]=Y\n",
    "Xcopy.to_csv(\"simple_vectors_with_labels.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da929cd-cf7e-4da5-bd8f-aa889044dea7",
   "metadata": {},
   "source": [
    "Train classifier time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45444c28-618b-4a00-ad8b-71cfdaa38aa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14457/2915915622.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my_trainn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "\n",
    "X_train = X\n",
    "y_trainn=Y\n",
    "start = time.time()\n",
    "\n",
    "model.fit(X_train, y_trainn)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74480371-c7de-42d8-9dd1-1e6a843e144f",
   "metadata": {},
   "source": [
    "## Enhance time ( (1) + NN + Query Time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9b903-30d8-4c3f-b842-7af7d72dd86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pandas as pd\n",
    "sparsevectors={}\n",
    "with open('text_vectors.pkl', 'rb') as f:\n",
    "    sparsevectors = pickle.load(f)\n",
    "    \n",
    "dfdesc = dfdesc.fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2218cbe8-3a3b-49ef-8fb8-0223fdf2c2ff",
   "metadata": {},
   "source": [
    "Time to build index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91211418-11ba-4dc1-af41-81f97a4a982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysparnn.cluster_index as ci\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "cp = ci.MultiClusterIndex(Xtfidf, dfdesc.index)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "with open('cluterIndex.bin', 'wb') as fh_out:\n",
    "    pickle.dump(cp, fh_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14677356-4786-4bcc-9f0f-217581675175",
   "metadata": {},
   "source": [
    "Query Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5a49398-1276-4448-98ae-1b01a8f84e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsimilartups(idn):\n",
    "    res=cp2.search(sparsevectors[idn], k=25, k_clusters=1, return_distance=True)\n",
    "    tups=[ (iddd[1],idn) for iddd in res[0][1:] if iddd[0]<0.90]\n",
    "    with open(\"new_edges.txt\",\"a+\") as f:\n",
    "        for tup in tups:\n",
    "            f.write(f\"{tup[0]},{tup[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85fff8e8-8ac4-4099-8170-be7a534cd65f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14457/3366474430.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cluterIndex.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcp2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'A'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('cluterIndex.bin', 'rb') as file_:\n",
    "    cp2 =pickle.load(file_)\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from multiprocessing import Pool\n",
    "    start = time.time()\n",
    "    with Pool(avpool) as p:\n",
    "        data=p.map(getsimilartups, list(sparsevectors.keys()))\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849064db-06dd-4490-9695-db55d15116b0",
   "metadata": {},
   "source": [
    "## building enhance graph vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c865f4b-dd64-4cb7-8dcd-ca3da935d68f",
   "metadata": {},
   "source": [
    "Build graph time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f061b-cd2c-4f36-8c57-ad41ba8bb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "sparsevectors={}\n",
    "with open('text_vectors.pkl', 'rb') as f:\n",
    "    sparsevectors = pickle.load(f)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_edges=pd.read_csv('train.csv')\n",
    "\n",
    "df_edges_all = df_edges[['id1', 'id2']]\n",
    "\n",
    "# load edges from tzt file (new_edges.txt)\n",
    "dfouredges=pd.read_csv(\"new_edges.txt\",header=None,names=[\"id1\",\"id2\"])\n",
    "\n",
    "dfouredges[\"id1\"]=dfouredges[\"id1\"].astype(int)\n",
    "dfouredges[\"id2\"]=dfouredges[\"id2\"].astype(int)\n",
    "\n",
    "df_edges_all=pd.concat([df_edges_all, dfouredges], ignore_index=True)\n",
    "\n",
    "\n",
    "nodes=list(sparsevectors.keys())\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "G= nx.Graph()\n",
    "#add nodes\n",
    "G.add_nodes_from(nodes)\n",
    "#add edges\n",
    "edges = [(row.id1,row.id2) for row in df_edges[df_edges[\"label\"]==1].itertuples()]\n",
    "G.add_edges_from(edges)\n",
    "edges2 = [(row.id1,row.id2) for row in dfouredges.itertuples()]\n",
    "G.add_edges_from(edges2)\n",
    "end = time.time()\n",
    "print(end -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab989b06-b5f1-41d0-976f-05d4481e23c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "# Adding methods proposed in the https://link.springer.com/content/pdf/10.1038/s41598-019-57304-y.pdf\n",
    "def shortest_path(G:nx.Graph,source:int,target:int)->float:\n",
    "    try:\n",
    "        p=nx.shortest_path(G,source=source,target=target)\n",
    "        return len(p)-1\n",
    "    except nx.NetworkXNoPath:\n",
    "        return float(\"inf\")\n",
    "\n",
    "def distance(G:nx.Graph,source:int,target:int)->float:\n",
    "   '''\n",
    "   This method can be changed to any distance\n",
    "   '''\n",
    "   return shortest_path(G,source,target)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def distance_description(G,source:int,target:int)->float:\n",
    "    '''\n",
    "    This method can be changed to any distance\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    sp1=sparsevectors[source]\n",
    "    sp2=sparsevectors[target]\n",
    "    \n",
    "    return cosine_similarity(sp1,sp2)[0][0]\n",
    "\n",
    "\n",
    "def closeness_centrality(G:nx.Graph, source:int, target:int)->float:\n",
    "    dxy = nx.shortest_path(G,source=source,target=target)\n",
    "    return G.number_of_nodes()/shortest_path(G,source,target)\n",
    "\n",
    "def common_neighbors(G:nx.Graph, source:int, target:int)->list:\n",
    "    s_neighbors = list(G.adj[source].keys())\n",
    "    t_neighbors = list(G.adj[target].keys())\n",
    "    common_neighbors = set(s_neighbors).intersection(t_neighbors)\n",
    "    return common_neighbors\n",
    "\n",
    "def CCPA(G:nx.Graph, source:int, target:int,a:float = 0.5)->float:\n",
    "    '''\n",
    "    Common Neighbor and Centrality based Parameterized Algorithm\n",
    "    '''\n",
    "    return a*closeness_centrality(G,source,target)+(1-a)*len(common_neighbors(G,source,target))\n",
    "\n",
    "def CND(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Common Neighbor and Distance\n",
    "    '''\n",
    "    cn = common_neighbors(G,source,target)\n",
    "    if len(cn)>0:\n",
    "        return (len(cn)+1)/2\n",
    "    else:\n",
    "        if distance(G,source,target)==0:\n",
    "            return 0\n",
    "        return 1/distance(G,source,target)\n",
    "\n",
    "def PA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Preferential Attachment\n",
    "    '''\n",
    "    return G.degree[source]*G.degree[target]\n",
    "\n",
    "def AA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Adamic Adar\n",
    "    '''\n",
    "    similarity = 0.0\n",
    "    for neighbor in common_neighbors(G,source,target):\n",
    "        degree = G.degree[neighbor]\n",
    "        if degree > 1:\n",
    "            similarity += 1 / math.log(degree)\n",
    "    return similarity\n",
    "\n",
    "def CN(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Common Neighbor\n",
    "    '''\n",
    "    return len(common_neighbors(G,source,target))\n",
    "\n",
    "def SI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Sorensen Index\n",
    "    '''\n",
    "    if (G.degree[source]+G.degree[target]) ==0:\n",
    "        return 0\n",
    "    return 2*CN(G,source,target)/(G.degree[source]+G.degree[target])\n",
    "\n",
    "def JI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Jaccard Index\n",
    "    '''\n",
    "    s_neighbors = list(G.adj[source].keys())\n",
    "    t_neighbors = list(G.adj[target].keys())\n",
    "    common_neighbors = set(s_neighbors).intersection(t_neighbors)\n",
    "    all_neighbors = set(s_neighbors).union(t_neighbors)\n",
    "    if len(all_neighbors)==0:\n",
    "        return 0\n",
    "    return len(common_neighbors)/len(all_neighbors)\n",
    "\n",
    "def RA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Resource Allocation\n",
    "    '''\n",
    "    similarity = 0.0\n",
    "    for neighbor in common_neighbors(G,source,target):\n",
    "        degree = G.degree[neighbor]\n",
    "        if degree > 1:\n",
    "            similarity += 1 / degree\n",
    "    return similarity\n",
    "\n",
    "def HPI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Hub Promoted Index\n",
    "    '''\n",
    "    if min([G.degree[source],G.degree[target]]) ==0 :\n",
    "        return 0\n",
    "    return CN(G,source,target)/min([G.degree[source],G.degree[target]])\n",
    "\n",
    "def create_metric_vector(tup):\n",
    "    G=tup[0]\n",
    "    source= tup[1]\n",
    "    target= tup[2]\n",
    "    functions = [CND, PA, AA, CN, SI, JI, RA, HPI]\n",
    "    outputs = []\n",
    "    for func in functions:\n",
    "        output = func(G, source, target)\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "\n",
    "def makedataset(df):\n",
    "    label=df.label\n",
    "    data=[]\n",
    "    allind=len(df.index)\n",
    "    counter=0\n",
    "    tups=[]\n",
    "    for i in tqdm(range(len(df.index))):\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        tups.append((G,row.id1,row.id2))\n",
    "    with Pool(avpool) as p:\n",
    "        data=p.map(create_metric_vector, tups)\n",
    "    \n",
    "    datadf=pd.DataFrame(data)\n",
    "    return datadf,label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd616f3-8caf-41ab-8a53-222c230e3e08",
   "metadata": {},
   "source": [
    "Build vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c033c-18e2-42c4-9312-33ff18689f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "X,Y=makedataset(df_edges)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "Xcopy=X.copy()\n",
    "Xcopy[\"label\"]=Y\n",
    "Xcopy.to_csv(\"enhance_vectors_with_labels.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d17510-35cc-4aff-af4c-53adb46d7618",
   "metadata": {},
   "source": [
    "train in enhaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69baa33-5c70-4e09-a551-355ac9211dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "start = time.time()\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "model.fit(X, Y)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
