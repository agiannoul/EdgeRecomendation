{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e18ffe50-9aa6-42da-bfb5-3b8f9dd49dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "avpool=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a4e428-dc33-4666-9f89-6858687d917d",
   "metadata": {},
   "source": [
    "# (1) Time for similarities in test descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a77eff7-d6f1-46cd-a587-6831db9ff49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle \n",
    "import pandas as pd\n",
    "sparsevectors={}\n",
    "with open('text_vectors.pkl', 'rb') as f:\n",
    "    sparsevectors = pickle.load(f)\n",
    "\n",
    "df_edges_test=pd.read_csv('test.csv')\n",
    "#df_edges_test= df_edges_test.iloc[:int(len(df_edges_test)*0.01)]\n",
    "\n",
    "def distance_description(tup)->float:\n",
    "    '''\n",
    "    This method can be changed to any distance\n",
    "    '''\n",
    "    source=tup[0]\n",
    "    target=tup[1]\n",
    "    sp1=sparsevectors[source]\n",
    "    sp2=sparsevectors[target]\n",
    "    \n",
    "    return cosine_similarity(sp1,sp2)[0][0]\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "my_preds=[]\n",
    "start = time.time()\n",
    "\n",
    "tups=[]\n",
    "for ind, row in tqdm(df_edges_test.iterrows()):\n",
    "    tups.append((row[\"id1\"],row[\"id2\"]))\n",
    "with Pool(avpool) as p:\n",
    "    data=p.map(distance_description, tups)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    dfsimilarities=pd.DataFrame(zip(df_edges_test[\"id\"],data),columns=[\"id\",\"similarity\"])\n",
    "    dfsimilarities.to_csv(\"test_similarities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a2c91-fb47-44b4-8999-09cdea2d802f",
   "metadata": {},
   "source": [
    "## (2) Vector Graph + predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "167c5cd8-acd1-4ced-856a-e711499ed2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import train dataset\n",
    "import pandas as pd\n",
    "\n",
    "df_edges_all=pd.read_csv('train.csv')\n",
    "df_edges_all_test=pd.read_csv('test.csv')\n",
    "\n",
    "\n",
    "df_edges= df_edges_all\n",
    "\n",
    "import pickle \n",
    "import pandas as pd\n",
    "sparsevectors={}\n",
    "with open('text_vectors.pkl', 'rb') as f:\n",
    "    sparsevectors = pickle.load(f)\n",
    "\n",
    "# The existing graph\n",
    "import networkx as nx\n",
    "\n",
    "G= nx.Graph()\n",
    "#add nodes\n",
    "nodestest=pd.unique(df_edges_all_test[['id1', 'id2']].values.ravel())\n",
    "nodes = pd.unique(df_edges_all[['id1', 'id2']].values.ravel())\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_nodes_from(nodestest)\n",
    "\n",
    "#add edges\n",
    "edges = [(row.id1,row.id2) for row in df_edges[df_edges[\"label\"]==1].itertuples()]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# keeping all the negative edges so we can compare results\n",
    "no_edges = [(row.id1,row.id2) for row in df_edges[df_edges[\"label\"]==0].itertuples()]\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "\n",
    "Xcopy=pd.read_csv(\"simple_vectors_with_labels.csv\")\n",
    "Y=Xcopy[\"label\"]\n",
    "X=Xcopy.drop([\"label\"],axis=1)\n",
    "model.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "808f8a6f-f1e0-40cc-8a56-f7c1c6c55f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Adding methods proposed in the https://link.springer.com/content/pdf/10.1038/s41598-019-57304-y.pdf\n",
    "def shortest_path(G:nx.Graph,source:int,target:int)->float:\n",
    "    try:\n",
    "        p=nx.shortest_path(G,source=source,target=target)\n",
    "        return len(p)-1\n",
    "    except nx.NetworkXNoPath:\n",
    "        return float(\"inf\")\n",
    "\n",
    "def distance(G:nx.Graph,source:int,target:int)->float:\n",
    "   '''\n",
    "   This method can be changed to any distance\n",
    "   '''\n",
    "   return shortest_path(G,source,target)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def distance_description(G,source:int,target:int)->float:\n",
    "    '''\n",
    "    This method can be changed to any distance\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    sp1=sparsevectors[source]\n",
    "    sp2=sparsevectors[target]\n",
    "    \n",
    "    return cosine_similarity(sp1,sp2)[0][0]\n",
    "\n",
    "\n",
    "def closeness_centrality(G:nx.Graph, source:int, target:int)->float:\n",
    "    dxy = nx.shortest_path(G,source=source,target=target)\n",
    "    return G.number_of_nodes()/shortest_path(G,source,target)\n",
    "\n",
    "def common_neighbors(G:nx.Graph, source:int, target:int)->list:\n",
    "    s_neighbors = list(G.adj[source].keys())\n",
    "    t_neighbors = list(G.adj[target].keys())\n",
    "    common_neighbors = set(s_neighbors).intersection(t_neighbors)\n",
    "    return common_neighbors\n",
    "\n",
    "def CCPA(G:nx.Graph, source:int, target:int,a:float = 0.5)->float:\n",
    "    '''\n",
    "    Common Neighbor and Centrality based Parameterized Algorithm\n",
    "    '''\n",
    "    return a*closeness_centrality(G,source,target)+(1-a)*len(common_neighbors(G,source,target))\n",
    "\n",
    "def CND(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Common Neighbor and Distance\n",
    "    '''\n",
    "    cn = common_neighbors(G,source,target)\n",
    "    if len(cn)>0:\n",
    "        return (len(cn)+1)/2\n",
    "    else:\n",
    "        if distance(G,source,target)==0:\n",
    "            return 0\n",
    "        return 1/distance(G,source,target)\n",
    "\n",
    "def PA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Preferential Attachment\n",
    "    '''\n",
    "    return G.degree[source]*G.degree[target]\n",
    "\n",
    "def AA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Adamic Adar\n",
    "    '''\n",
    "    similarity = 0.0\n",
    "    for neighbor in common_neighbors(G,source,target):\n",
    "        degree = G.degree[neighbor]\n",
    "        if degree > 1:\n",
    "            similarity += 1 / math.log(degree)\n",
    "    return similarity\n",
    "\n",
    "def CN(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Common Neighbor\n",
    "    '''\n",
    "    return len(common_neighbors(G,source,target))\n",
    "\n",
    "def SI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Sorensen Index\n",
    "    '''\n",
    "    if (G.degree[source]+G.degree[target]) ==0:\n",
    "        return 0\n",
    "    return 2*CN(G,source,target)/(G.degree[source]+G.degree[target])\n",
    "\n",
    "def JI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Jaccard Index\n",
    "    '''\n",
    "    s_neighbors = list(G.adj[source].keys())\n",
    "    t_neighbors = list(G.adj[target].keys())\n",
    "    common_neighbors = set(s_neighbors).intersection(t_neighbors)\n",
    "    all_neighbors = set(s_neighbors).union(t_neighbors)\n",
    "    if len(all_neighbors)==0:\n",
    "        return 0\n",
    "    return len(common_neighbors)/len(all_neighbors)\n",
    "\n",
    "def RA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Resource Allocation\n",
    "    '''\n",
    "    similarity = 0.0\n",
    "    for neighbor in common_neighbors(G,source,target):\n",
    "        degree = G.degree[neighbor]\n",
    "        if degree > 1:\n",
    "            similarity += 1 / degree\n",
    "    return similarity\n",
    "\n",
    "def HPI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Hub Promoted Index\n",
    "    '''\n",
    "    if min([G.degree[source],G.degree[target]]) ==0 :\n",
    "        return 0\n",
    "    return CN(G,source,target)/min([G.degree[source],G.degree[target]])\n",
    "\n",
    "def create_metric_vector(tup):\n",
    "    G=tup[0]\n",
    "    source= tup[1]\n",
    "    target= tup[2]\n",
    "    functions = [CND, PA, AA, CN, SI, JI, RA, HPI]\n",
    "    outputs = []\n",
    "    for func in functions:\n",
    "        output = func(G, source, target)\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def makedataset_TEST(df):\n",
    "    data=[]\n",
    "    allind=len(df.index)\n",
    "    counter=0\n",
    "    tups=[]\n",
    "    for i in tqdm(range(len(df.index))):\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        tups.append((G,row.id1,row.id2))\n",
    "    with Pool(avpool) as p:\n",
    "        data=p.map(create_metric_vector, tups)\n",
    "    \n",
    "    datadf=pd.DataFrame(data)\n",
    "    return datadf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c1bc9-6234-42ce-9db2-9d74093389d7",
   "metadata": {},
   "source": [
    "create vector time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0ffd4b-1743-4fde-9b66-6a4cd175697e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 238364/238364 [00:11<00:00, 19865.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.77423429489136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "X_test=makedataset_TEST(df_edges_all_test)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca59368-17b2-4db1-b33c-a86e77f87808",
   "metadata": {},
   "source": [
    "Predict Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fcd0165-5a81-4888-9cba-397a964169fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05102729797363281\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "preds = model.predict(X_test)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b126f535-c7e5-4bee-a461-038a3b0904d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexpe=pd.DataFrame(zip(df_edges_all_test[\"id\"].values,preds),columns=[\"id\",\"label\"])\n",
    "dfexpe.to_csv(\"simple_graph_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a240a9-94fe-4f6c-9061-e8e964cb34e2",
   "metadata": {},
   "source": [
    "## (3) Enhance test vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d32a9039-cbdd-4534-894c-94d39a51fc7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'enhance_vectors_with_labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18380/3659784612.py\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mXcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"enhance_vectors_with_labels.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXcopy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'enhance_vectors_with_labels.csv'"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "sparsevectors={}\n",
    "with open('text_vectors.pkl', 'rb') as f:\n",
    "    sparsevectors = pickle.load(f)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_edges=pd.read_csv('train.csv')\n",
    "\n",
    "df_edges_all = df_edges[['id1', 'id2']]\n",
    "\n",
    "# load edges from tzt file (new_edges.txt)\n",
    "dfouredges=pd.read_csv(\"new_edges.txt\",header=None,names=[\"id1\",\"id2\"])\n",
    "\n",
    "dfouredges[\"id1\"]=dfouredges[\"id1\"].astype(int)\n",
    "dfouredges[\"id2\"]=dfouredges[\"id2\"].astype(int)\n",
    "\n",
    "df_edges_all=pd.concat([df_edges_all, dfouredges], ignore_index=True)\n",
    "\n",
    "\n",
    "nodes=list(sparsevectors.keys())\n",
    "\n",
    "\n",
    "\n",
    "G= nx.Graph()\n",
    "#add nodes\n",
    "G.add_nodes_from(nodes)\n",
    "#add edges\n",
    "edges = [(row.id1,row.id2) for row in df_edges[df_edges[\"label\"]==1].itertuples()]\n",
    "G.add_edges_from(edges)\n",
    "edges2 = [(row.id1,row.id2) for row in dfouredges.itertuples()]\n",
    "G.add_edges_from(edges2)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "\n",
    "Xcopy=pd.read_csv(\"enhance_vectors_with_labels.csv\")\n",
    "Y=Xcopy[\"label\"]\n",
    "X=Xcopy.drop([\"label\"],axis=1)\n",
    "model.fit(X_train, y_trainn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890fe05-049f-4c66-92d2-d42e493b5605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "# Adding methods proposed in the https://link.springer.com/content/pdf/10.1038/s41598-019-57304-y.pdf\n",
    "def shortest_path(G:nx.Graph,source:int,target:int)->float:\n",
    "    try:\n",
    "        p=nx.shortest_path(G,source=source,target=target)\n",
    "        return len(p)-1\n",
    "    except nx.NetworkXNoPath:\n",
    "        return float(\"inf\")\n",
    "\n",
    "def distance(G:nx.Graph,source:int,target:int)->float:\n",
    "   '''\n",
    "   This method can be changed to any distance\n",
    "   '''\n",
    "   return shortest_path(G,source,target)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def distance_description(G,source:int,target:int)->float:\n",
    "    '''\n",
    "    This method can be changed to any distance\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    sp1=sparsevectors[source]\n",
    "    sp2=sparsevectors[target]\n",
    "    \n",
    "    return cosine_similarity(sp1,sp2)[0][0]\n",
    "\n",
    "\n",
    "def closeness_centrality(G:nx.Graph, source:int, target:int)->float:\n",
    "    dxy = nx.shortest_path(G,source=source,target=target)\n",
    "    return G.number_of_nodes()/shortest_path(G,source,target)\n",
    "\n",
    "def common_neighbors(G:nx.Graph, source:int, target:int)->list:\n",
    "    s_neighbors = list(G.adj[source].keys())\n",
    "    t_neighbors = list(G.adj[target].keys())\n",
    "    common_neighbors = set(s_neighbors).intersection(t_neighbors)\n",
    "    return common_neighbors\n",
    "\n",
    "def CCPA(G:nx.Graph, source:int, target:int,a:float = 0.5)->float:\n",
    "    '''\n",
    "    Common Neighbor and Centrality based Parameterized Algorithm\n",
    "    '''\n",
    "    return a*closeness_centrality(G,source,target)+(1-a)*len(common_neighbors(G,source,target))\n",
    "\n",
    "def CND(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Common Neighbor and Distance\n",
    "    '''\n",
    "    cn = common_neighbors(G,source,target)\n",
    "    if len(cn)>0:\n",
    "        return (len(cn)+1)/2\n",
    "    else:\n",
    "        if distance(G,source,target)==0:\n",
    "            return 0\n",
    "        return 1/distance(G,source,target)\n",
    "\n",
    "def PA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Preferential Attachment\n",
    "    '''\n",
    "    return G.degree[source]*G.degree[target]\n",
    "\n",
    "def AA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Adamic Adar\n",
    "    '''\n",
    "    similarity = 0.0\n",
    "    for neighbor in common_neighbors(G,source,target):\n",
    "        degree = G.degree[neighbor]\n",
    "        if degree > 1:\n",
    "            similarity += 1 / math.log(degree)\n",
    "    return similarity\n",
    "\n",
    "def CN(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Common Neighbor\n",
    "    '''\n",
    "    return len(common_neighbors(G,source,target))\n",
    "\n",
    "def SI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Sorensen Index\n",
    "    '''\n",
    "    if (G.degree[source]+G.degree[target]) ==0:\n",
    "        return 0\n",
    "    return 2*CN(G,source,target)/(G.degree[source]+G.degree[target])\n",
    "\n",
    "def JI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Jaccard Index\n",
    "    '''\n",
    "    s_neighbors = list(G.adj[source].keys())\n",
    "    t_neighbors = list(G.adj[target].keys())\n",
    "    common_neighbors = set(s_neighbors).intersection(t_neighbors)\n",
    "    all_neighbors = set(s_neighbors).union(t_neighbors)\n",
    "    if len(all_neighbors)==0:\n",
    "        return 0\n",
    "    return len(common_neighbors)/len(all_neighbors)\n",
    "\n",
    "def RA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Resource Allocation\n",
    "    '''\n",
    "    similarity = 0.0\n",
    "    for neighbor in common_neighbors(G,source,target):\n",
    "        degree = G.degree[neighbor]\n",
    "        if degree > 1:\n",
    "            similarity += 1 / degree\n",
    "    return similarity\n",
    "\n",
    "def HPI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Hub Promoted Index\n",
    "    '''\n",
    "    if min([G.degree[source],G.degree[target]]) ==0 :\n",
    "        return 0\n",
    "    return CN(G,source,target)/min([G.degree[source],G.degree[target]])\n",
    "\n",
    "def create_metric_vector(tup):\n",
    "    G=tup[0]\n",
    "    source= tup[1]\n",
    "    target= tup[2]\n",
    "    functions = [CND, PA, AA, CN, SI, JI, RA, HPI]\n",
    "    outputs = []\n",
    "    for func in functions:\n",
    "        output = func(G, source, target)\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def makedataset_TEST(df):\n",
    "    data=[]\n",
    "    allind=len(df.index)\n",
    "    counter=0\n",
    "    tups=[]\n",
    "    for i in tqdm(range(len(df.index))):\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        tups.append((G,row.id1,row.id2))\n",
    "    with Pool(avpool) as p:\n",
    "        data=p.map(create_metric_vector, tups)\n",
    "    \n",
    "    datadf=pd.DataFrame(data)\n",
    "    return datadf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe5518-854a-4447-8f86-fc78031d8ef1",
   "metadata": {},
   "source": [
    "Enhance vectors time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4053ec4e-7022-4aab-b6ba-55693a17b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges_all_test=pd.read_csv('test.csv')\n",
    "\n",
    "start = time.time()\n",
    "X_test=makedataset_TEST(df_edges_all_test)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a2aee-ee24-462b-bde2-709451fc75dc",
   "metadata": {},
   "source": [
    "enhance predict time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849a335-682f-4a9d-a6b3-3cd0b3c13805",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "preds = model.predict(X_test)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
