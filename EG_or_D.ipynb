{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d45c871b-2dab-4874-bd22-438e5889b297",
   "metadata": {},
   "source": [
    "This method represents our proposed solution, where we combine the similarity calculated using the TF-IDF vectors (D) with the graph vectors derived from the enhanced graph (EG). We predict the existence of a link if at least one of the two methods reports it as a link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ac437-c096-42df-aea9-84c1aebc4d4e",
   "metadata": {},
   "source": [
    "## Offline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366cd93-9d87-46bd-ac07-846cf15ee539",
   "metadata": {},
   "source": [
    "### Requires:\n",
    "#### text_vectors.pkl and test_similarities.csv: which are produced by runing D.ipynb\n",
    "#### new_edges.txt and enhance_vectors_with_labels.csv : which are produced by runing EG.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e1f6a-3bff-4f7c-9055-2b3aff4cba4b",
   "metadata": {},
   "source": [
    "Load Tf-Idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a786c56-f7c3-472a-80de-bb0bb5af1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pandas as pd\n",
    "sparsevectors={}\n",
    "with open('text_vectors.pkl', 'rb') as f:\n",
    "    sparsevectors = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b63ec-43fd-4db8-9373-58a4e8181ad7",
   "metadata": {},
   "source": [
    "Build the enhanced graph using new_edges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2e227d0-e833-411e-b744-c678dad9cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "df_edges=pd.read_csv('train.csv')\n",
    "\n",
    "df_edges_all = df_edges[['id1', 'id2']]\n",
    "\n",
    "\n",
    "dfouredges=pd.read_csv(\"new_edges.txt\",header=None,names=[\"id1\",\"id2\"])\n",
    "\n",
    "dfouredges[\"id1\"]=dfouredges[\"id1\"].astype(int)\n",
    "dfouredges[\"id2\"]=dfouredges[\"id2\"].astype(int)\n",
    "\n",
    "df_edges_all=pd.concat([df_edges_all, dfouredges], ignore_index=True)\n",
    "\n",
    "\n",
    "nodes=list(sparsevectors.keys())\n",
    "\n",
    "\n",
    "\n",
    "G= nx.Graph()\n",
    "#add nodes\n",
    "G.add_nodes_from(nodes)\n",
    "#add edges\n",
    "edges = [(row.id1,row.id2) for row in df_edges[df_edges[\"label\"]==1].itertuples()]\n",
    "G.add_edges_from(edges)\n",
    "edges2 = [(row.id1,row.id2) for row in dfouredges.itertuples()]\n",
    "G.add_edges_from(edges2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b2fb1-96d0-488b-bc3d-47bfc97029b8",
   "metadata": {},
   "source": [
    "Define graph metrics (same as G and EG solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad7320f9-44f7-43e8-8705-0c2d2304f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Adding methods proposed in the https://link.springer.com/content/pdf/10.1038/s41598-019-57304-y.pdf\n",
    "def shortest_path(G:nx.Graph,source:int,target:int)->float:\n",
    "    try:\n",
    "        p=nx.shortest_path(G,source=source,target=target)\n",
    "        return len(p)-1\n",
    "    except nx.NetworkXNoPath:\n",
    "        return float(\"inf\")\n",
    "\n",
    "def distance(G:nx.Graph,source:int,target:int)->float:\n",
    "   '''\n",
    "   This method can be changed to any distance\n",
    "   '''\n",
    "   return shortest_path(G,source,target)\n",
    "\n",
    "def closeness_centrality(G:nx.Graph, source:int, target:int)->float:\n",
    "    dxy = nx.shortest_path(G,source=source,target=target)\n",
    "    return G.number_of_nodes()/shortest_path(G,source,target)\n",
    "\n",
    "def common_neighbors(G:nx.Graph, source:int, target:int)->list:\n",
    "    s_neighbors = list(G.adj[source].keys())\n",
    "    t_neighbors = list(G.adj[target].keys())\n",
    "    common_neighbors = set(s_neighbors).intersection(t_neighbors)\n",
    "    return common_neighbors\n",
    "\n",
    "def CCPA(G:nx.Graph, source:int, target:int,a:float = 0.5)->float:\n",
    "    '''\n",
    "    Common Neighbor and Centrality based Parameterized Algorithm\n",
    "    '''\n",
    "    return a*closeness_centrality(G,source,target)+(1-a)*len(common_neighbors(G,source,target))\n",
    "\n",
    "def CND(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Common Neighbor and Distance\n",
    "    '''\n",
    "    cn = common_neighbors(G,source,target)\n",
    "    if len(cn)>0:\n",
    "        return (len(cn)+1)/2\n",
    "    else:\n",
    "        if distance(G,source,target)==0:\n",
    "            return 0\n",
    "        return 1/distance(G,source,target)\n",
    "\n",
    "def PA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Preferential Attachment\n",
    "    '''\n",
    "    return G.degree[source]*G.degree[target]\n",
    "\n",
    "def AA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Adamic Adar\n",
    "    '''\n",
    "    similarity = 0.0\n",
    "    for neighbor in common_neighbors(G,source,target):\n",
    "        degree = G.degree[neighbor]\n",
    "        if degree > 1:\n",
    "            similarity += 1 / math.log(degree)\n",
    "    return similarity\n",
    "\n",
    "def CN(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Common Neighbor\n",
    "    '''\n",
    "    return len(common_neighbors(G,source,target))\n",
    "\n",
    "def SI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Sorensen Index\n",
    "    '''\n",
    "    if (G.degree[source]+G.degree[target]) ==0:\n",
    "        return 0\n",
    "    return 2*CN(G,source,target)/(G.degree[source]+G.degree[target])\n",
    "\n",
    "def JI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Jaccard Index\n",
    "    '''\n",
    "    s_neighbors = list(G.adj[source].keys())\n",
    "    t_neighbors = list(G.adj[target].keys())\n",
    "    common_neighbors = set(s_neighbors).intersection(t_neighbors)\n",
    "    all_neighbors = set(s_neighbors).union(t_neighbors)\n",
    "    if len(all_neighbors)==0:\n",
    "        return 0\n",
    "    return len(common_neighbors)/len(all_neighbors)\n",
    "\n",
    "def RA(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Resource Allocation\n",
    "    '''\n",
    "    similarity = 0.0\n",
    "    for neighbor in common_neighbors(G,source,target):\n",
    "        degree = G.degree[neighbor]\n",
    "        if degree > 1:\n",
    "            similarity += 1 / degree\n",
    "    return similarity\n",
    "\n",
    "def HPI(G:nx.Graph, source:int, target:int)->float:\n",
    "    '''\n",
    "    Hub Promoted Index\n",
    "    '''\n",
    "    if min([G.degree[source],G.degree[target]]) ==0 :\n",
    "        return 0\n",
    "    return CN(G,source,target)/min([G.degree[source],G.degree[target]])\n",
    "\n",
    "def create_metric_vector(tup):\n",
    "    G=tup[0]\n",
    "    source= tup[1]\n",
    "    target= tup[2]\n",
    "    functions = [CND, PA, AA, CN, SI, JI, RA, HPI]\n",
    "    outputs = []\n",
    "    for func in functions:\n",
    "        output = func(G, source, target)\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def makedatasetapply(df):\n",
    "    label=df.label\n",
    "    datadf = df.progress_apply(lambda row: create_metric_vector((G,row['id1'],row['id2'])), axis=1,result_type='expand')\n",
    "    return datadf,label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a7f16-f080-4b63-99f3-25ebf41d2604",
   "metadata": {},
   "source": [
    "load vectors for training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0066206-2ab5-44d2-90fc-11e9d2b0c235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "\n",
    "Xcopy=pd.read_csv(\"enhance_vectors_with_labels.csv\")\n",
    "Y=Xcopy[\"label\"]\n",
    "X=Xcopy.drop([\"label\"],axis=1)\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625a027-b7c0-4020-b802-50740f3bf8d7",
   "metadata": {},
   "source": [
    "## Online"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce494d2-83ae-452d-a085-98069f4c54e6",
   "metadata": {},
   "source": [
    "Create test vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f527b6c7-8b70-4f58-8f55-7cdabaca9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedataset_TEST(df):\n",
    "    datadf = df.apply(lambda row: create_metric_vector((G,row['id1'],row['id2'])), axis=1,result_type='expand')\n",
    "    return datadf\n",
    "\n",
    "df_edges_all_test=pd.read_csv('test.csv')\n",
    "X_test=makedataset_TEST(df_edges_all_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab3f71-144e-49dd-a082-b0a01101e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcopy=X_test.copy()\n",
    "Xcopy.to_csv(\"enhance_vectors_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f9c69-9fc2-40de-a97f-9fc70ed18b03",
   "metadata": {},
   "source": [
    "produce predictions using similarites and vectors of testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fa300-6e65-4d9d-b330-988c7d9d1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcopy=pd.read_csv(\"enhance_vectors_test.csv\")\n",
    "\n",
    "X_test=Xcopy\n",
    "\n",
    "predsEG = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e0de6-f11e-47e5-ae4e-e00a6bc70730",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsimilarities=pd.read_csv(\"test_similarities.csv\")\n",
    "predd = [int(d>0.08) for d in dfsimilarities[\"similarity\"]]\n",
    "\n",
    "preds = [p or d for p,d in zip(predsEG,predd)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf3561-eadf-4f61-8242-ca59ab445410",
   "metadata": {},
   "source": [
    "Save submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805997e-95bb-4e32-b341-e71671c9f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexpe=pd.DataFrame(zip(df_edges_all_test[\"id\"].values,preds),columns=[\"id\",\"label\"])\n",
    "dfexpe.to_csv(\"enhance_graph_plus_similarities_submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
